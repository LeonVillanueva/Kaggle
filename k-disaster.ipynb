{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['id', 'keyword', 'location', 'text', 'target'], dtype=object),\n",
       " array(['id', 'keyword', 'location', 'text'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values, test.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train['location'].notnull()] # see samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      match\n",
       "0     0        #\n",
       "3     0        #\n",
       "4     0        #\n",
       "      1        #\n",
       "5     0        #\n",
       "              ..\n",
       "7599  1        #\n",
       "7601  0        #\n",
       "      1        #\n",
       "7604  0        #\n",
       "7607  0        #\n",
       "Name: 0, Length: 3403, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].str.extractall(r'(\\#)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = '#hello World 20 is on Fire #fire #world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbolcount (x,s='#'):\n",
    "    return len(x.split (s)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ulratio (x):\n",
    "    n = sum(1 for l in x if l.isupper())\n",
    "    d = sum(1 for l in x if l.islower())\n",
    "    if d > 0:\n",
    "        return n/d\n",
    "    else:\n",
    "        return 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nratio (x):\n",
    "    n = sum(1 for l in x if l.isnumeric())\n",
    "    d = len(x)\n",
    "    return n/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['text'].apply (symbolcount, args=('@'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download ('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "    x = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", x)\n",
    "    stop_words = stopwords.words ('english')\n",
    "    words = re.sub(r'[^\\w\\s]','',x).lower().split(' ')\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_text(train['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer (x):\n",
    "    assert isinstance(x, pd.DataFrame), 'Input (x) not a DataFrame'\n",
    "    \n",
    "    x['text_clean'] = x['text'].apply(clean_text)\n",
    "    x['ratio'] = x['text'].apply(ulratio)\n",
    "    x['nratio'] = x['text'].apply(nratio)\n",
    "    x['hashcount'] = x['text'].apply(symbolcount, args=('#'))\n",
    "    x['atcount'] = x['text'].apply(symbolcount, args=('@'))\n",
    "    x['length'] = x['text'].apply(len)/280\n",
    "    x['key'] = ~x['keyword'].isna()\n",
    "    x['geo'] = ~x['location'].isna()\n",
    "    x['key'] = x['key'] * 1\n",
    "    x['geo'] = x['geo'] * 1\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = transformer (train)\n",
    "test = transformer (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_train['target'] = revtarget (r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>ratio</th>\n",
       "      <th>nratio</th>\n",
       "      <th>hashcount</th>\n",
       "      <th>atcount</th>\n",
       "      <th>length</th>\n",
       "      <th>key</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>0.018692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean     ratio  \\\n",
       "0       1  [deeds, reason, earthquake, may, allah, forgiv...  0.217391   \n",
       "1       1      [forest, fire, near, la, ronge, sask, canada]  0.192308   \n",
       "2       1  [residents, asked, shelter, place, notified, o...  0.018692   \n",
       "3       1  [13000, people, receive, wildfires, evacuation...  0.020408   \n",
       "4       1  [got, sent, photo, ruby, alaska, smoke, wildfi...  0.044776   \n",
       "\n",
       "     nratio  hashcount  atcount    length  key  geo  \n",
       "0  0.000000          1        0  0.246429    0    0  \n",
       "1  0.000000          0        0  0.135714    0    0  \n",
       "2  0.000000          0        0  0.475000    0    0  \n",
       "3  0.076923          1        0  0.232143    0    0  \n",
       "4  0.000000          2        0  0.314286    0    0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train.to_csv(path_or_buf='train_r.csv', index=False)\n",
    "test.to_csv(path_or_buf='test.csv', index=False)\n",
    "train.to_csv(path_or_buf='train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[['text_clean','ratio', 'nratio', 'hashcount', 'atcount', 'length', 'key', 'geo']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 60000\n",
    "tweet = 280\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer (num_words = max_vocab_size)\n",
    "tokenizer.fit_on_texts (train['text_clean'].values)\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences (train['text_clean'])\n",
    "sequence_train = pad_sequences (sequence_train, maxlen=tweet, padding='post')\n",
    "sequence_test = tokenizer.texts_to_sequences (test['text_clean'])\n",
    "sequence_test = pad_sequences (sequence_test, maxlen=tweet, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train = train[['ratio', 'nratio', 'hashcount', 'atcount', 'length', 'key', 'geo']].values\n",
    "meta_test = test[['ratio', 'nratio', 'hashcount', 'atcount', 'length', 'key', 'geo']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 7), (7613, 280))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train.shape, sequence_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  np.concatenate((sequence_train,meta_train),axis=1)\n",
    "X_test =  np.concatenate((sequence_test,meta_test),axis=1)\n",
    "y_train = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tt = sequence_train.shape[1]\n",
    "Tm = meta_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "*https://towardsdatascience.com/how-to-enter-your-first-kaggle-competition-4717e7b232db*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_words (x):\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgd = train['text_clean'].apply(join_words)\n",
    "test_sgd = test['text_clean'].apply(join_words)\n",
    "\n",
    "train_target_sgd = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613,), (7613,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sgd.shape, train_target_sgd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_sgd, X_test_sgd, y_train_sgd, y_test_sgd = train_test_split(train_sgd, train_target_sgd, random_state = 23)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipeline_xgd = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('xgc', XGBClassifier()),\n",
    "])\n",
    "\n",
    "model_xgb = pipeline_xgd.fit(X_train_sgd, y_train_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgb = model_xgb.predict_proba (test_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgb = predict_xgb[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7185172212492703, 0.016939140040076094)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "accuracies = cross_val_score(estimator = model_xgb, X = X_train_sgd, y = y_train_sgd, cv = 10)\n",
    "accuracies.mean(), accuracies.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma (x):\n",
    "    return [lemmatizer.lemmatize (word) for word in x if not word.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemma = train['text_clean'].apply (lemma)\n",
    "test_lemma = test['text_clean'].apply (lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size_l = 40000\n",
    "\n",
    "tokenizer_l = Tokenizer (num_words = max_vocab_size_l)\n",
    "tokenizer_l.fit_on_texts (train_lemma.values)\n",
    "\n",
    "sequence_train_lemma = tokenizer.texts_to_sequences (train_lemma)\n",
    "sequence_train_lemma = pad_sequences (sequence_train, maxlen=tweet, padding='post')\n",
    "sequence_test_lemma = tokenizer.texts_to_sequences (test_lemma)\n",
    "sequence_test_lemma = pad_sequences (sequence_test, maxlen=tweet, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tl = sequence_train_lemma.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Reshape, Embedding, LSTM, GlobalMaxPooling1D, concatenate, MaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = tokenizer.word_index\n",
    "V = len (word_to_idx) # (V)ocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = tokenizer_l.word_index\n",
    "Vl = len (word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64 # embedding dimension\n",
    "\n",
    "input_t = Input (shape=(Tt,))\n",
    "input_l = Input (shape=(Tl,))\n",
    "input_m = Input (shape=(Tm,))\n",
    "\n",
    "\n",
    "w = Embedding (V+1, D) (input_l)\n",
    "w = Dropout (0.5) (w)\n",
    "w = Bidirectional (LSTM (64, return_sequences=True)) (w)\n",
    "w = BatchNormalization () (w)\n",
    "w = GlobalMaxPooling1D () (w)\n",
    "w = Dropout (0.5) (w)\n",
    "w = Dense (32, activation='relu') (w)\n",
    "w = Model (inputs=input_l, outputs=w)\n",
    "\n",
    "x = Embedding (V+1, D) (input_t)\n",
    "x = Dropout (0.5) (x)\n",
    "x = Bidirectional (LSTM (128, return_sequences=True)) (x)\n",
    "x = BatchNormalization () (x)\n",
    "x = GlobalMaxPooling1D () (x)\n",
    "x = Dropout (0.5) (x)\n",
    "x = Dense (64, activation='relu') (x)\n",
    "x = Model (inputs=input_t, outputs=x)\n",
    "\n",
    "y = Dense (32, activation=\"relu\") (input_m)\n",
    "y = Dropout (0.5) (y)\n",
    "y = Dense (8, activation=\"relu\") (y)\n",
    "y = Model (inputs=input_m, outputs=y)\n",
    "\n",
    "combined = concatenate([x.output, w.output, y.output])\n",
    "\n",
    "z = Dense (96, activation=\"relu\") (combined)\n",
    "z = Dropout (0.5) (z)\n",
    "z = Dense (32, activation=\"relu\") (z)\n",
    "z = Dense (1, activation=\"sigmoid\") (z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[x.input, w.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam (learning_rate=0.005, decay=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile (optimizer=adam,\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7613 samples\n",
      "Epoch 1/24\n",
      "7613/7613 [==============================] - 19s 2ms/sample - loss: 0.6653 - accuracy: 0.5952\n",
      "Epoch 2/24\n",
      "7613/7613 [==============================] - 10s 1ms/sample - loss: 0.4422 - accuracy: 0.8189\n",
      "Epoch 3/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.2815 - accuracy: 0.9031\n",
      "Epoch 4/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.1811 - accuracy: 0.9345\n",
      "Epoch 5/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.1379 - accuracy: 0.9506\n",
      "Epoch 6/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.1106 - accuracy: 0.9590\n",
      "Epoch 7/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0960 - accuracy: 0.9614\n",
      "Epoch 8/24\n",
      "7613/7613 [==============================] - 10s 1ms/sample - loss: 0.0798 - accuracy: 0.9679\n",
      "Epoch 9/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0725 - accuracy: 0.9703\n",
      "Epoch 10/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0702 - accuracy: 0.9718\n",
      "Epoch 11/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0674 - accuracy: 0.9722\n",
      "Epoch 12/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0644 - accuracy: 0.9739\n",
      "Epoch 13/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0602 - accuracy: 0.9744\n",
      "Epoch 14/24\n",
      "7613/7613 [==============================] - 10s 1ms/sample - loss: 0.0594 - accuracy: 0.9746\n",
      "Epoch 15/24\n",
      "7613/7613 [==============================] - 10s 1ms/sample - loss: 0.0592 - accuracy: 0.9754\n",
      "Epoch 16/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0509 - accuracy: 0.9774\n",
      "Epoch 17/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0535 - accuracy: 0.9753\n",
      "Epoch 18/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0554 - accuracy: 0.9748\n",
      "Epoch 19/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0465 - accuracy: 0.9786\n",
      "Epoch 20/24\n",
      "7613/7613 [==============================] - 10s 1ms/sample - loss: 0.0501 - accuracy: 0.9781\n",
      "Epoch 21/24\n",
      "7613/7613 [==============================] - 10s 1ms/sample - loss: 0.0456 - accuracy: 0.9792\n",
      "Epoch 22/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0408 - accuracy: 0.9787\n",
      "Epoch 23/24\n",
      "7613/7613 [==============================] - 10s 1ms/sample - loss: 0.0456 - accuracy: 0.9769\n",
      "Epoch 24/24\n",
      "7613/7613 [==============================] - 9s 1ms/sample - loss: 0.0469 - accuracy: 0.9791\n"
     ]
    }
   ],
   "source": [
    "epochs = 24\n",
    "\n",
    "r = model.fit ([sequence_train, sequence_train_lemma, meta_train], y_train, epochs=epochs, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tf = model.predict ([sequence_test, sequence_test_lemma, meta_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = np.round (predict_tf)\n",
    "test['target'] = test['target'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','target']].to_csv(path_or_buf='result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_combo = np.add ((predict_xgb.reshape(-1,1)*.1), (predict_tf*.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = np.round (predict_combo)\n",
    "test['target'] = test['target'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','target']].to_csv(path_or_buf='result_combo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = np.round (predict_xgb.reshape(-1,1))\n",
    "test['target'] = test['target'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','target']].to_csv(path_or_buf='result_xgb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
